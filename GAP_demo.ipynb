{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low-complexity Near-optimum Symbol Detection Based on Neural Enhancement of Factor Graphs\n",
    "---\n",
    "Luca Schmid and Laurent Schmalen\n",
    "\n",
    "We consider the application of the factor graph framework for symbol detection on linear inter-symbol interference channels. Based on the Ungerboeck observation model, a detection algorithm with appealing complexity properties can be derived. However, since the underlying factor graph contains cycles, the sum-product algorithm (SPA) yields a suboptimal algorithm. In this paper, we develop and evaluate efficient strategies to improve the performance of the factor graph-based symbol detection by means of neural enhancement. In particular, we consider neural belief propagation and generalizations of the factor nodes as an effective way to mitigate the effect of cycles within the factor graph. By applying a generic preprocessor to the channel output, we propose a simple technique to vary the underlying factor graph in every SPA iteration. Using this dynamic factor graph transition, we intend to preserve the extrinsic nature of the SPA messages which is otherwise impaired due to cycles. Simulation results show that the proposed methods can massively improve the detection performance, even approaching the maximum a posteriori performance for various transmission scenarios, while preserving a complexity which is linear in both the block length and the channel memory.\n",
    "\n",
    "The full paper [1] is available on arXiv and IEEE Xplore.\n",
    "\n",
    "This jupyter notebook shows an examplary implementation and usage of the GAP algorithm. The code is implemented close to the description in [1]. The comments within the source code assume prior knowledge in the field of message apssing on factor graphs and assume that you have completely studied the ideas and notations in [1]. In specific, this notebook contains:\n",
    "* Generic implementation of the GAP algorithm, parallelized to handle multiple batches of data blocks in parallel. The class is implemented with pyTorch and can run on CPUs or GPUs.\n",
    "* Example training and evaluation procedure with configurable parameters.\n",
    "* Some helper classes (to handle bit2symbol mappings, bit-metric decoding, etc.) and loss functions (BMI and BER).\n",
    "\n",
    "[1] Schmid, Luca, and Laurent Schmalen. “Low-Complexity Near-Optimum Symbol Detection Based on Neural Enhancement of Factor Graphs.” ArXiv:2203.16417 [Cs, Eess, Math], March 30, 2022. http://arxiv.org/abs/2203.16417.\n",
    "\n",
    "For questions, discussions or improvements of this code, feel free to contact us (Luca Schmid, first.last@kit.edu) or open an issue/create a pull request. Have fun!\n",
    "\n",
    "---\n",
    "\n",
    "This work has received funding in part from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 101001899) and in part from the German Federal Ministry of Education and Research (BMBF) within the project Open6GHub (grant agreement 16KISK010).\n",
    "\n",
    "---\n",
    "\n",
    "Copyright (c) 2021-2022 Luca Schmid - Communications Engineering Lab (CEL), Karlsruhe Institute of Technology (KIT)\n",
    "\n",
    "<sup> Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "<sup> The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "<sup> THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import external libraries\n",
    "import numpy as np\n",
    "import torch as t\n",
    "device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some common symbol constellations for digital baseband transmission.\n",
    "bpsk_mapping = t.tensor([1.0, -1.0], dtype=t.cfloat)\n",
    "qpsk_mapping = 1/np.sqrt(2) * t.tensor([-1-1j, -1+1j, 1-1j, 1+1j], dtype=t.cfloat)\n",
    "qam16_mapping = 1/np.sqrt(10) * t.tensor([-3-3j, -3-1j, -3+3j, -3+1j, # uses Gray coding\n",
    "                                          -1-3j, -1-1j, -1+3j, -1+1j,\n",
    "                                          +3-3j, +3-1j, +3+3j, +3+1j,\n",
    "                                          +1-3j, +1-1j, +1+3j, +1+1j ], dtype=t.cfloat)\n",
    "\n",
    "# Define impules response of some example channels.\n",
    "ProakisA = [0.04, -0.05, 0.07, -0.21, -0.5, 0.72, 0.36, 0.0, 0.21, 0.03, 0.07]\n",
    "ProakisB = [0.407, 0.815, 0.407]\n",
    "ProakisC = [0.227, 0.460, 0.688, 0.460, 0.277]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constellation helper class.\n",
    "class constellation:\n",
    "    \"\"\"\n",
    "    Class which provides some functions, applied to an arbitrary complex constellation, given by a bit2symbol mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mapping, device):\n",
    "        \"\"\"\n",
    "        :param mapping: t.Tensor which contains the constellation symbols, \n",
    "            sorted according to their binary representation (MSB first).\n",
    "        :param device: Device on which the tensors are allocated, e.g., \"cuda\", or \"cpu\".\n",
    "        \"\"\"\n",
    "        assert len(mapping.shape) == 1 # mapping should be a 1-dim tensor\n",
    "        self.mapping = mapping.to(device)\n",
    "         \n",
    "        self.M = t.numel(mapping) # M is number of constellation symbols\n",
    "        assert self.M > 1\n",
    "        self.m = np.log2(self.M).astype(int) # m=ld(M) is number of bits\n",
    "        assert self.m == np.log2(self.M) # Assert that log2(M) is integer\n",
    "        \n",
    "        # Some helpers or bit-metric decoding etc.\n",
    "        self.mask = 2 ** t.arange(self.m - 1, -1, -1).to(device)\n",
    "        self.sub_consts = t.stack([t.stack([t.arange(self.M).reshape(2**(i+1),-1)[::2].flatten(), t.arange(self.M).reshape(2**(i+1),-1)[1::2].flatten()]) for i in range(self.m)]).to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def map(self, bits):\n",
    "        \"\"\"\n",
    "        Maps a given bit_sequence to a sequence of constellation symbols.\n",
    "        The length of the output sequence is len(bit_sequence) / m.\n",
    "        The operation is applied to the last axis of bit_sequences.\n",
    "        bit_sequence is allowed to have other dimensions (e.g. multiple sequences at once)\n",
    "        as long as the last dimensions is the sequence.\n",
    "        \"\"\"\n",
    "        # Assert that the length of the bit sequence is a multiple of m.\n",
    "        in_shape = bits.shape\n",
    "        assert in_shape[-1]/self.m == in_shape[-1]//self.m\n",
    "        # Reshape and convert bits to decimal and use decimal number as index for mapping.\n",
    "        return self.mapping[t.sum(self.mask * bits.reshape(in_shape[:-1] + (-1, self.m)), -1)]\n",
    "    \n",
    "    def bit_metric_decoder(self, symbol_apps):\n",
    "        \"\"\"\n",
    "        Receives a sequence of symbol probabilities/beliefs. \n",
    "        For each symbol, an M-dim tensor indicates the logarithmic probability for each of the M possible \n",
    "        constellation symbols.\n",
    "        The bit metric decoder calculates the bit LLRs for each of the m bits for each symbol.\n",
    "        \"\"\"\n",
    "        assert len(symbol_apps.shape) >= 2 # dim -2: symbol sequence, dim -1: M log APPs\n",
    "        assert symbol_apps.shape[-1] == self.M\n",
    "\n",
    "        # For each of the m bits, repartition the M APPs into two subsets regarding the respective bit.\n",
    "        # The output vector has shape (..., m, 2, M/2).\n",
    "        if self.M > 2:\n",
    "            subset_probs = t.index_select(symbol_apps,-1, self.sub_consts.flatten()).view(symbol_apps.shape[:-1] + self.sub_consts.shape)\n",
    "            # Sum up probabilities of all subsets (in log domain).\n",
    "            bitwise_apps = self.jacobian_sum(subset_probs, dim=-1)\n",
    "            # Compute LLR.\n",
    "            LLR = (bitwise_apps[...,0] - bitwise_apps[...,1]).flatten(start_dim = -2)\n",
    "        else: # M==2 -> we only have one binary channel -> bit-metric decoder is trivial\n",
    "            LLR = symbol_apps[...,0] - symbol_apps[...,1]\n",
    "            \n",
    "        assert symbol_apps.shape[:-2] == LLR.shape[:-1]\n",
    "        assert symbol_apps.shape[-2]*self.m == LLR.shape[-1]\n",
    "        assert not t.isinf(LLR).any()\n",
    "        return LLR\n",
    "    \n",
    "    def jacobian_sum(self, msg, dim):\n",
    "        \"\"\"\n",
    "        Computes ln(e^a_1 + e^a_2 + ... e^a_M) of a tensor with last dimension (a_1, a_2, ..., a_M)\n",
    "        by applying the Jacobian algorithm. (Also called log-sum-exp operation.)\n",
    "        \"\"\"\n",
    "        assert msg.shape[dim] > 1\n",
    "        if dim == -1:\n",
    "            return t.max(msg, dim=-1)[0] + t.log(t.sum(t.exp(msg - t.max(msg, dim=-1, keepdim=True)[0]), dim=-1))\n",
    "        elif dim == -2:\n",
    "            return t.max(msg, dim=-2)[0] + t.log(t.sum(t.exp(msg - t.max(msg, dim=-2, keepdim=True)[0]), dim=-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the GAP algorithm\n",
    "class GAP(nn.Module):\n",
    "    \"\"\"\n",
    "    Symbol detection algorithm for linear ISI channels based on the sum-product algorithm (SPA) \n",
    "    on a generalized factor graph. For details, see [1].\n",
    "\n",
    "    Notes: * All computations are carried out in the log domain.\n",
    "           * In contrast to the very general description in [1], we drastically reduce the number of\n",
    "             trainable parameters (the nn.Parameter objects, associated with the Boolean flags weight_Inm, \n",
    "             weight_F and nbp) by choosing them to be independent of the index 'k', i.e., along the dimension of the \n",
    "             block length (which can typically be quite large). From our experience, this introduces only a minor\n",
    "             performance degradation but significantly slims down the training and gives space to increase other, \n",
    "             more relevant paramters, like B or S.\n",
    "    \"\"\"\n",
    "    def __init__(self, block_len, channel_taps, constellation, \n",
    "                       pfilter, branches=1, stages=1, iters=5, \n",
    "                       device='cpu',\n",
    "                       weight_Inm=False, weight_F=False, learn_pfilter=False, \n",
    "                       nbp=False, weight_priors=False\n",
    "                       ):\n",
    "        \"\"\"\n",
    "        :param block_len: Number of transmit symbols per information block.\n",
    "        :param channel_taps: L+1 channel taps of the impulse response of the linear ISI channel with memory L.\n",
    "        :param constellation: Constellation object, defining a mapping of M constellation symbols on m = ld(M) bits.\n",
    "        :param pfilter: Preprocessing filter. The filter must have at least the length of the\n",
    "            channel filter. If it is longer, the number of additional taps compared to the channel\n",
    "            must be even. By choosing a filter matched to the channel, the Ungerboeck observation model is applied.\n",
    "            If more than 1 stage and/or branch is used, you can input multiple (initial) pfilters\n",
    "            in the shape [stages, branches, pfilter length]. \n",
    "            Otherwise, the one pfilter is used for all stages/branches.\n",
    "        :param branches: Number of parallel branches (called B in [1]). The resulting outputs of the different \n",
    "            branches are combined after each stage according to eq. (14) in [1]. \n",
    "            Each branch may uses a different pfilter.\n",
    "        :param stages: Number of dynamic factor graph transitions (called S in [1]). Each stage s uses the output\n",
    "            of the previous stage as input to initialize its messages.\n",
    "            Each stage s may use a different pfilter.\n",
    "        :param iters: Number of iterations (called N' in [1]) for the SPA on the factor graph for each stage and branch.\n",
    "            One iteration contains a variable2factor (V2F) node update and a factor2variable (F2V) update\n",
    "            wit full parallel / flooding schedule.\n",
    "        :param device: Device on which the tensors are allocated, e.g., \"cuda\", or \"cpu\".\n",
    "        :param weight_Inm: Boolean flag. If True, the factors I_kl(c_k,c_l) are weighted with trainable\n",
    "            parameters. See Sec. IV.A in [1].\n",
    "        :param weight_F: Boolean flag. If True, the factors F_k(c_k) are weighted with trainable params.\n",
    "            See Sec. IV.A in [1].\n",
    "        :param learn_pfilter: Boolean flag. If True, the preprocessing filters P are learned. \n",
    "            They are initialized with the given impulse responses, given by the tensor pfilter. \n",
    "            Compare IV.B in [1]. If False, the filters, given by the tensor pfilter, are used but not adapted/optimized.\n",
    "        :param nbp: Boolean flag. If True, all messages are parametrized with a multiplicative weight. \n",
    "            Compare IV.A in [1].\n",
    "        :param weight_priors: If True, the priors which are fed to the individual equalizers\n",
    "            are weighted with a parametrizable weight. Compare IV.B in [1].\n",
    "        \"\"\"\n",
    "        super(GAP, self).__init__()\n",
    "        # Process input params.\n",
    "        self.device = device # device on which the computations are carried out\n",
    "        assert iters > 0\n",
    "        self.iters = iters # number of iterations of the SPA in each stage/branch\n",
    "        assert stages > 0\n",
    "        self.stages = stages # number of factor graph transitions (called S in [1])\n",
    "        assert branches > 0\n",
    "        self.branches = branches # number of parallel detetors (called B in [1])\n",
    "\n",
    "        # channel specifics\n",
    "        assert len(channel_taps.shape) == 1 and len(channel_taps) > 1 # implementation-specific restrictions\n",
    "        self.h = channel_taps # channel impulse response\n",
    "        self.l = len(channel_taps) - 1 # memory of the channel\n",
    "        self.const = constellation # constellation object\n",
    "        self.n = block_len # number of transmit symbols N\n",
    "        self.k = block_len + self.l # number of receive symbols K (length of y in [1])\n",
    "        \n",
    "        # preprocessing filter\n",
    "        if len(pfilter.shape) == 1: # only one pfilter for all branches and stages is given\n",
    "            pfilters = pfilter.repeat(stages,branches,1)\n",
    "        else:\n",
    "            pfilters = pfilter\n",
    "            assert len(pfilters.shape) == 3\n",
    "            assert pfilters.shape[:2] == (stages, branches,)\n",
    "        self.p_len = pfilters.shape[-1]\n",
    "        assert self.p_len >= len(channel_taps) # This is only a restriction for this specific implementation.\n",
    "        assert (self.p_len - len(channel_taps)) % 2 == 0\n",
    "        self.os_extend = (self.p_len - len(channel_taps)) // 2\n",
    "        # L2 is the new memory of the overall filter (channel + preprocessor). This is equivalent to L+L_p in [1].\n",
    "        self.l2 = self.l + self.os_extend \n",
    "        # Assign preprocessing filter to a tensor or parameter, depending if it is being optimized or if it is fixed.\n",
    "        if learn_pfilter:\n",
    "            # individual filter for each stage and branch\n",
    "            self.p = t.nn.Parameter(pfilters)\n",
    "        else:\n",
    "            # constant preprocessing filter\n",
    "            self.p = pfilters.to(self.device) \n",
    "\n",
    "        # Boolean flags, specifying what we want to optimized.\n",
    "        self.learn_pfilter = learn_pfilter\n",
    "        self.weight_Inm = weight_Inm\n",
    "        self.weight_F = weight_F\n",
    "        self.nbp = nbp\n",
    "        self.weight_priors = weight_priors\n",
    "\n",
    "        # Initial computation of the G matrix and the Inm factors (both independent of observed data).\n",
    "        # This only needs to be changed, if the preprocessing filter is adapted.\n",
    "        self.Gnn, Gnm = self.compute_G()\n",
    "        self.Inm = self.compute_Inm(Gnm)\n",
    "\n",
    "        # Init nn.Parameters for trainable parameters.\n",
    "        if self.weight_Inm:\n",
    "            # 1 scalar weight for the Inm factor in each iteration, stage and branch.\n",
    "            self.Inm_weight = t.nn.Parameter(t.ones((iters,stages,branches, 2*self.l2), device=device))\n",
    "\n",
    "        if weight_F:\n",
    "            #  2 scalar weights for the F factor in each iteration (+ init), stage and branch.\n",
    "            self.F_weight = t.nn.Parameter(t.ones((self.iters+1, stages, branches, 2), device=device))\n",
    "\n",
    "        if nbp:\n",
    "            # Individual weights per iteration and port, but not for each of the N symbols.\n",
    "            self.v2f_weights = t.nn.Parameter(t.ones((iters,stages, branches, 2*self.l2), device=device))\n",
    "            self.f2v_weights = t.nn.Parameter(t.ones((iters,stages, branches, 2*self.l2), device=device))\n",
    "\n",
    "        if self.weight_priors:\n",
    "            # 1 scalar weight for each stage, branch and iterations, to weight the influence of the previous output.\n",
    "            self.prior_weight = t.nn.Parameter(t.ones((iters+1, stages, branches), device=device))\n",
    "\n",
    "    def compute_G(self):\n",
    "        \"\"\"\n",
    "        Computes the matrix G = PH.\n",
    "        Since the matrix multiplication represents a convolution, G has a band structure.\n",
    "        Gnn outputs the diagonal value of G (all diagonal elements are equal) \n",
    "        and Gnm outputs one band (=row without zeros) of G.\n",
    "        \n",
    "        :returns: Tensor Gnn of shape [stages, branches].\n",
    "        :returns: Tensor Gnm of shape [stages, branches, 2 L2].\n",
    "        \n",
    "        Note: At the moment, when this class was written, func.conv1d did not allow complex convolution.\n",
    "              If you have a complex-valued channel and/or preprocessing filter, you need to adapt this function\n",
    "              (either convolve RE and IM part separately, or use complex convolution, if provided by torch.)\n",
    "        \"\"\"\n",
    "        # Convolve P and H to compute Gnm. \n",
    "        conv = func.conv1d(t.flip(self.p, dims=[-1]).view(self.branches*self.stages,1,self.p_len), \n",
    "                           self.h.repeat(1,1,1), \n",
    "                           padding=self.l).view(self.stages,self.branches,2*self.l2 + 1)\n",
    "        \n",
    "        # The convolution of P and H gives the values of the bands of G. \n",
    "        Gnm = t.zeros((self.stages,self.branches,2*self.l2), device=self.device)\n",
    "        Gnm[:,:, : self.l2] = conv[:,:, : self.l2]\n",
    "        Gnn = conv[:,:, self.l2].to(self.device)\n",
    "        Gnm[:,:, self.l2:] = conv[:,:, self.l2+1 : ] \n",
    "        return Gnn, Gnm\n",
    "\n",
    "    def compute_Inm(self, Gnm):\n",
    "        \"\"\"\n",
    "        Compute parts of the factors I_kl(c_k,c_l): Inm = Re[Gnm cm cn]\n",
    "        The Eb/N0 weight is multiplied in each specific batch, as the Eb/N0 (and possible trainable weights) \n",
    "        might vary over the iterations/training steps.\n",
    "\n",
    "        :returns: Tensor of shape [stages, branches, 2 L2, M, M].\n",
    "        \"\"\"\n",
    "        return -t.real(Gnm[:,:, :,None,None] * \n",
    "                self.const.mapping[None,None,None,None,:] * t.conj(self.const.mapping[None,None,None,:,None]))\n",
    "\n",
    "    def forward(self, y, EsN0_lin, priors=None):\n",
    "        \"\"\"\n",
    "        Accept channel observation y and apply symbol detection algorithm, as described in [1, Algorithm 1].\n",
    "        The basic structure is:\n",
    "        * Apply SPA on all parallel branches of one stage.\n",
    "            * Each branch builds up an individual factor graph, by applying its preprocessing filter to the observation y.\n",
    "            * Iteratively apply F2V and V2F update rule and pass messages between VNs and FNs (on each branch independently).\n",
    "        * Merge results of all branches and pass results to the next stage.\n",
    "        \n",
    "        :param y: Channel observation of shape [batch_size, K].\n",
    "        :param EsN0_lin: Es/N0 in linear domain, for each batch individually. Shape [batch_size].\n",
    "        :returns: Logarithmic beliefs of each symbol. Shape [batch_size, N, M].\n",
    "        \"\"\"\n",
    "        assert y.shape[0] == EsN0_lin.shape[0] # Check for equal batch sizes.\n",
    "        \n",
    "        # Compute G (if pfilter is learned, G may change within the lifetime of the detector).\n",
    "        self.Gnn, Gnm = self.compute_G()\n",
    "        self.Inm = self.compute_Inm(Gnm)\n",
    "        \n",
    "        # Apply preprocessing filters of all stages/branches in parallel.\n",
    "        x = self.compute_x(y) # x has shape [batch_size, stages, branches, n].\n",
    "\n",
    "        # Apply equalizer stages serially.\n",
    "        for stage_i in range(self.stages):\n",
    "            # Compute initial V2F message, based on (possibly preprocessed) observation.\n",
    "            v2f_msg = self.compute_init_msg(x, EsN0_lin, priors, stage_i) # shape [batch_size, branches, stages, N, 2L_2, M]\n",
    "            # Iteratively update factor nodes and variable nodes, based on SPA update rules.\n",
    "            for i in range(self.iters):\n",
    "                f2v_msg = self.FN_update(v2f_msg, EsN0_lin, iteration=i, stage=stage_i)\n",
    "                v2f_msg, indiv_apps = self.VN_update(f2v_msg, x, EsN0_lin, iteration=i, stage=stage_i, priors=priors)\n",
    "            \n",
    "            # Combine individual outputs of all branches as input for next stage.\n",
    "            assert indiv_apps.shape == (y.shape[0], self.branches, self.n, self.const.M)\n",
    "            # Normalize to true logarithmic probabilities.\n",
    "            normed_apps = indiv_apps - (self.jacobian_sum(indiv_apps, dim=-1))[...,None] \n",
    "            combi = t.sum(normed_apps, dim=1) # Combine APPs of all branches.\n",
    "            # Normalize again after combi.\n",
    "            priors = combi - (self.jacobian_sum(combi, dim=-1))[...,None] \n",
    "        return priors\n",
    "\n",
    "    def FN_update(self, v2f_msg, EsN0_lin, iteration, stage):\n",
    "        \"\"\"\n",
    "        Receives v2f messages. A row holds all messages outgoing from a VN.\n",
    "        1.) Resort, so that a row holds all (future) ingoing messages to one FN.\n",
    "        2.) Apply FN update rule (add Inm and aply Jacobian algorithm (log-sum-exp)).\n",
    "\n",
    "        :param v2f_msg: Messages from VNs to FNs in the shape [batch_size, branches, N, 2 L2, M].\n",
    "        :param iterations: Current iteration in which we are (relevant for iteration-specific parameters).\n",
    "        :param stage: Current stage in which we are (relevant for stage-specific paramters).\n",
    "        :returns: Updated messages from factor to variable nodes of shape [batch_size, branches, N, 2 L2, M].\n",
    "        \"\"\"\n",
    "        assert v2f_msg.shape[1:] == (self.branches, self.n, 2*self.l2, self.const.M)\n",
    "        #1.) Resort, so that all incoming messages to one FN are in the same dimension -2.\n",
    "        v2f = t.zeros(v2f_msg.shape, dtype=float, device=self.device)\n",
    "        for i in range(self.l2):\n",
    "            v2f[:,:,self.l2-i:,i,:] = v2f_msg[:,:,:self.n-self.l2+i,-1-i,:]\n",
    "            v2f[:,:,:self.n-self.l2+i,-1-i,:] = v2f_msg[:,:,self.l2-i:,i,:]\n",
    "\n",
    "        #2.) Weight v2f messages (if enabled), add factor and marginalize out one dimension.\n",
    "        if self.weight_Inm:\n",
    "            Inm = self.Inm[stage,:,:,:,:] * self.Inm_weight[iteration, stage,:,:,None,None]\n",
    "        else:\n",
    "            Inm = self.Inm[stage]\n",
    "        if self.nbp:\n",
    "            core = (self.v2f_weights[iteration,stage,None,:,None,:,None] * v2f)[:,:,:,:,None,:] + \\\n",
    "                    EsN0_lin[:,None,None,None,None,None] * Inm[None,:,None,:,:,:]\n",
    "        else: # no weights\n",
    "            core = v2f[:,:,:,:,None,:] + \\\n",
    "                   EsN0_lin[:,None,None,None,None,None] * Inm[None,:,None,:,:,:] \n",
    "        del v2f\n",
    "\n",
    "        # After epxansion and multiplication with Inm, core has the shape [batch, branch, n, 2L_2, M1, M2]\n",
    "        # Marginalize out the last dimension from the incident message.\n",
    "        f2v = self.jacobian_sum(core, dim=-1)\n",
    "        del core\n",
    "\n",
    "        # Normalize, so that maximum value is 0 (not an actual normalization to probabilities, only for numerical stability).\n",
    "        f2v_normed = f2v - (t.max(f2v, dim=-1)[0])[...,None] \n",
    "        del f2v\n",
    "\n",
    "        # Weight f2v messages (NBP).\n",
    "        if self.nbp:\n",
    "            return f2v_normed * self.f2v_weights[iteration,None,stage,:,None,:,None]\n",
    "        else:\n",
    "            return f2v_normed\n",
    "\n",
    "    def VN_update(self, f2v_msg, x, EsN0_lin, iteration, stage, priors):\n",
    "        \"\"\"\n",
    "        Sum up all incoming messages (also msg from F) -> single beliefs.\n",
    "        For outgoing (extrinsic) messages, subtract intrinsic message, respectively.\n",
    "        \n",
    "        :param f2v_msg: Messages from factor to variable nodes of shape [batch_size, branches, N, 2 L2, M].\n",
    "            Each dimension -2 holds all messages incoming to one specific variable node.\n",
    "        :param x: Preprocessed observation of shape [batch_size, stages, branches, N].\n",
    "        :EsN0_lin: Es/N0 in linear domain, for each batch individually. Shape [batch_size].\n",
    "        :param iterations: Current iteration in which we are (relevant for iteration-specific parameters).\n",
    "        :param stage: Current stage in which we are (relevant for stage-specific paramters).\n",
    "        :param priors: Prior information (either a priori information about statistics, or info from previous stage).\n",
    "        \n",
    "        :returns: Updated messages from variable to factornodes of shape [batch_size, branches, N, 2 L2, M].\n",
    "        :returns: Current beliefs about each variable node in each branch of shape [batch_size, branches, N, M].\n",
    "        \"\"\"\n",
    "        assert f2v_msg.shape[1:] == (self.branches, self.n, 2*self.l2, self.const.M)\n",
    "        # Sum up all incoming messages.\n",
    "        beliefs = t.sum(f2v_msg, dim=-2) + self.compute_F(x, EsN0_lin, iteration=iteration+1, stage=stage)\n",
    "        # Apply weights on messages (NBP).\n",
    "        if priors != None:\n",
    "            assert priors.shape == (beliefs.shape[0], self.n, self.const.M)\n",
    "            if self.weight_priors:\n",
    "                beliefs += self.prior_weight[iteration+1, stage,None,:,None,None] * priors[:,None,:,:]\n",
    "            else:\n",
    "                beliefs += priors[:,None,:,:]\n",
    "        v2f_msg = beliefs[:,:,:,None,:] - f2v_msg # beliefs has shape [batch, branch, N, M].\n",
    "        \n",
    "        return v2f_msg, beliefs\n",
    "\n",
    "    def compute_x(self, y):\n",
    "        \"\"\"\n",
    "        Apply preprocessor (e.g., matched filter), for each branch and stage.\n",
    "        \n",
    "        :param y: Channel observation of shape [batch_size, K].\n",
    "        :returns: x of shape [batch_size, stages, branches, N(= K-L = block length)]\n",
    "        \n",
    "        Note: At the moment, when this class was written, func.conv1d did not allow complex convolution.\n",
    "        \"\"\"\n",
    "        batch_size = y.shape[0]\n",
    "\n",
    "        # Matched filter output of the observation.\n",
    "        assert len(y.shape) == 2 and y.shape[1] == self.k\n",
    "\n",
    "        x =(1.0 * func.conv1d(y.real.view(batch_size,1,self.k), t.flip(self.p, dims=[-1]).view(self.branches*self.stages,1,-1), padding=self.os_extend) + \n",
    "            1.0j* func.conv1d(y.imag.view(batch_size,1,self.k), t.flip(self.p, dims=[-1]).view(self.branches*self.stages,1,-1), padding=self.os_extend)).view(batch_size,self.stages, self.branches,self.n)\n",
    "        return x\n",
    "\n",
    "    def compute_F(self, x, EsN0_lin, iteration, stage):\n",
    "        \"\"\"\n",
    "        Compute F factor (depending on the channel observation x, the SNR, the constellation  and Gnn).\n",
    "        \n",
    "        :param x: Preprocessed observation of shape [batch_size, stages, branches, N].\n",
    "        :EsN0_lin: Es/N0 in linear domain, for each batch individually. Shape [batch_size].\n",
    "        :param iterations: Current iteration in which we are (relevant for iteration-specific parameters).\n",
    "        :param stage: Current stage in which we are (relevant for stage-specific paramters).\n",
    "        \n",
    "        :returns: factor F in log domain of shape [batch_size, branches, N, M]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        if self.weight_F:\n",
    "            F = EsN0_lin[:,None,None,None] * t.real((self.F_weight[iteration,None,stage,:,None,0] * x[:,stage,:,:])[...,None] * t.conj(self.const.mapping)[None,None,None,:] - \n",
    "                    (self.F_weight[iteration,stage,:,None,1] * ((self.Gnn/2)[stage,:,None] * (t.abs(self.const.mapping)**2)[None,:]))[None,:,None,:] )\n",
    "        else:\n",
    "            F = EsN0_lin[:,None,None,None] * t.real(x[:,stage,:,:,None] * t.conj(self.const.mapping)[None,None,None,:] - \n",
    "                    ((self.Gnn/2)[stage,:,None] * (t.abs(self.const.mapping)**2)[None,:])[None,:,None,:] )\n",
    "        return F.view(batch_size, self.branches, self.n, self.const.M)\n",
    "\n",
    "    def compute_init_msg(self, x, EsN0_lin, priors, stage):\n",
    "        \"\"\" \n",
    "        Compute messages to initialize the message passing.\n",
    "        Compute F and broadcast F+priors (both optionally weighted) to all 2*L2 ports. \n",
    "        \"\"\"\n",
    "        F = self.compute_F(x, EsN0_lin, iteration=0, stage=stage).view((-1, self.branches,self.n, 1, self.const.M))\n",
    "        # F has shape [batch_size, branches, n, 1, M]\n",
    "        batch_size = F.shape[0]\n",
    "        if priors == None: # If no priors are fed, we only use the channel observation, assuming uniformly distributed symbols.       \n",
    "            return F.repeat(1,1,1,2*self.l2,1) # Repeat msg as broadcast to all factor nodes.\n",
    "        else:\n",
    "            assert priors.shape == (batch_size, self.n, self.const.M)\n",
    "            if self.weight_priors:\n",
    "                return (F + (self.prior_weight[0,None,stage,:,None,None,None] * priors[:,None,:,None,:])) \\\n",
    "                        .repeat(1,1,1,2*self.l2,1) # Repeat msg as broadcast to all factor nodes.\n",
    "            else:\n",
    "                return (F + priors[:,None,None,:,:]).view(\n",
    "                        batch_size,self.stages,self.branches,self.n,1,self.const.M).repeat(1,1,1,1,2*self.l2,1) # Repeat msg as broadcast to all factor nodes.\n",
    "\n",
    "    def jacobian_sum(self, msg, dim):\n",
    "        \"\"\"\n",
    "        Computes ln(e^a_1 + e^a_2 + ... e^a_M) of a tensor with last dimension (a_1, a_2, ..., a_M)\n",
    "        by applying the Jacobian algorithm. (Also called log-sum-exp operation.)\n",
    "        \"\"\"\n",
    "        assert msg.shape[dim] > 1\n",
    "        if dim == -1:\n",
    "            return t.max(msg, dim=-1)[0] + t.log(t.sum(t.exp(msg - t.max(msg, dim=-1, keepdim=True)[0]), dim=-1))\n",
    "        elif dim == -2:\n",
    "            return t.max(msg, dim=-2)[0] + t.log(t.sum(t.exp(msg - t.max(msg, dim=-2, keepdim=True)[0]), dim=-2))\n",
    "\n",
    "    def save_weights(self, filename: str):\n",
    "        \"\"\"\n",
    "        Helper function, to save all trainable paramters.\n",
    "        :param filename: Path (as character string) to the directory where to save the parameters.\n",
    "        \"\"\"\n",
    "        if self.weight_Inm:\n",
    "            t.save(self.Inm_weight, filename+\"Inm_weight.pt\")\n",
    "        if self.weight_F:\n",
    "            t.save(self.F_weight, filename+\"F.pt\")\n",
    "        if self.learn_pfilter:\n",
    "            t.save(self.p, filename+\"pfilter.pt\")\n",
    "        if self.nbp:\n",
    "            t.save(self.v2f_weights, filename+\"v2f.pt\")\n",
    "            t.save(self.f2v_weights, filename+\"f2v.pt\")\n",
    "        if self.weight_priors:\n",
    "            t.save(self.prior_weight, filename+\"prior_weight.pt\")\n",
    "        \n",
    "    def load_weights(self, filename: str):\n",
    "        \"\"\"\n",
    "        Helper function, to load all trainable parameters from files.\n",
    "        :param filename: Path (as character string) to the directory where to load the parameters from.\n",
    "        \"\"\"\n",
    "        if self.weight_Inm:\n",
    "            self.Inm_weight = t.nn.Parameter(t.load(filename+\"Inm_weight.pt\", map_location=self.device))\n",
    "        if self.weight_F:\n",
    "            self.F_weight = t.nn.Parameter(t.load(filename+\"F.pt\", map_location = self.device))\n",
    "        if self.learn_pfilter:\n",
    "            self.p = t.nn.Parameter(t.load(filename+\"pfilter.pt\", map_location = self.device))\n",
    "        if self.nbp:\n",
    "            self.v2f_weights = t.nn.Parameter(t.load(filename+\"v2f.pt\", map_location = self.device))\n",
    "            self.f2v_weights = t.nn.Parameter(t.load(filename+\"f2v.pt\", map_location = self.device))\n",
    "        if self.weight_priors:\n",
    "            self.prior_weight = t.nn.Parameter(t.load(filename+\"prior_weight.pt\", map_location = self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for simulations\n",
    "def detect(EbN0_dB_min, EbN0_dB_max):\n",
    "    \"\"\"\n",
    "    Randomly generates data batch, simulates channel and runs equalization algorithm.\n",
    "    The Eb/N0 is sampled from a uniform distribution in (EbN0_dB_min, EbN0_dB_max), assuming that the\n",
    "    average power of the applied constellation is normalized to 1.\n",
    "    \n",
    "    :returns: beliefs of algorithm (approximated APPs).\n",
    "    :returns: label bits\n",
    "    \"\"\"\n",
    "    # Compute Es/N0 (lin) from Eb/N0 (dB).\n",
    "    EsN0_lin_min = 10 ** (EbN0_dB_min / 10) * const.m\n",
    "    EsN0_lin_max = 10 ** (EbN0_dB_max / 10) * const.m\n",
    "    \n",
    "    # Simulate channel.\n",
    "    bits = t.randint(2, size=(batch_size, block_len*const.m)).to(device) # Generate random bits.\n",
    "    tx = const.map(bits) # Map bits to symbols.\n",
    "    # Convolve with channel.\n",
    "    rx =(      t.nn.functional.conv1d(tx.real.view(batch_size,1,block_len), t.flip(channel, dims=[0]).view(1,1,-1), padding=l) + \\\n",
    "         1.0j* t.nn.functional.conv1d(tx.imag.view(batch_size,1,block_len), t.flip(channel, dims=[0]).view(1,1,-1), padding=l)).view(batch_size, block_len+l)\n",
    "\n",
    "    # Decide for noise level for each individual batch (uniformly distributed).\n",
    "    EsN0_lin_batches = ((EsN0_lin_max - EsN0_lin_min) * t.rand(batch_size) + EsN0_lin_min).to(device)\n",
    "    rx += t.randn((batch_size, block_len+l), dtype=t.cfloat, device=device) / t.sqrt(EsN0_lin_batches[:,None])\n",
    "    # Run factor graph equalizer. \n",
    "    return eq(rx, EsN0_lin_batches), bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure the GAP algorithm and the transmisison scenario here:**\n",
    "* Transmission scenario: constellation, channel impulse response, block length\n",
    "* GAP parameters: \n",
    "  * Number of branches B, stages S and SPA iterations \n",
    "  * Preprocessing filter(s), e.g., matched filter in case of the Ungerboeck observation model\n",
    "  * Boolean flags to specifically dis/enable trainable parameters\n",
    "  \n",
    "  \n",
    "Note: to recreate the original factor graph-based detection algorithm based on the Ungerboeck model [2] (which serves as a basis for the GAP algorithm), parametrize the GAP algorithm with B=S=1, use a matched filter as preprocessor and disable all Boolean flags.\n",
    "\n",
    "[2] Colavolpe, Giulio, Dario Fertonani, and Amina Piemontese. “SISO Detection over Linear Channels with Linear Complexity in the Number of Interferers.” IEEE Journal of Selected Topics in Signal Processing 5, no. 8 (December 2011): 1475–85. https://doi.org/10.1109/JSTSP.2011.2168943."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "# Transmission scenario\n",
    "const = constellation(bpsk_mapping, device) # Select constellation.\n",
    "channel = t.tensor(ProakisB, device=device) # Select channel.\n",
    "l = len(channel)-1 # channel memory\n",
    "block_len = 100 # Number of information symbols per transmission block\n",
    "\n",
    "# Algorithm parameters\n",
    "branches = 1 # Number of parallel branches, the algorithm uses (called B in [1])\n",
    "stages = 1 # Number of serial stages/ dynamic factor graph transitions (called S in [1])\n",
    "iters = 6 # SPA iterations per stage and branch (called N' in [1])\n",
    "# Choose preprocessing filter(s). The original Ungerboeck model uses a matched filter, the GAP\n",
    "p_filter = t.flip(channel, dims=[0]) # matched filter\n",
    "#p_filter = t.randn((stages, branches, 7), device=device) # (Initial) preprocessing filter\n",
    "\n",
    "# Instantiate equalizer. Specify which parts of the algorithm should be optimized in the following training precedure, \n",
    "# by setting the respective flags.\n",
    "eq = GAP(block_len = block_len, channel_taps = channel, constellation = const,\n",
    "         pfilter=p_filter, branches = branches, stages = stages, iters=iters,\n",
    "         device=device,\n",
    "         weight_Inm=True, weight_F=True, learn_pfilter=False, nbp=True, weight_priors=True\n",
    "         )\n",
    "# Load parameters with eq.load_weights() here, if desired. Otherwise they are initialized with 1.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some objective functions to evaluate detection performance and for training.\n",
    "\n",
    "def BER(log_app, label_bits, constellation):\n",
    "    \"\"\"\n",
    "    Computes the (hard-decision) bit error rate (BER) for a soft-output equalizer with given labeled data.\n",
    "\n",
    "    :param log_app: A posterior probabilities of the symbols in logarithmic domain.\n",
    "    :param label_bits: The actual transmitted bits. The label of the data batch.\n",
    "    :returns 1-element tensor (scalar) BER value of the labeled data set, averaged over the complete batch.\n",
    "    \"\"\"\n",
    "    # Assert matching shapes of result data and label data.\n",
    "    assert log_app.shape[-2] == label_bits.shape[-1]/constellation.m\n",
    "    assert log_app.shape[-1] == constellation.M # The last shape should be the log-probabilities of the symbols.\n",
    "    llrs = constellation.bit_metric_decoder(log_app)\n",
    "    return t.sum(t.where(llrs > 0, 0, 1) != label_bits) / label_bits.numel() # Return bit error rate.\n",
    "\n",
    "def BMI(log_app, label_bits, constellation):\n",
    "    \"\"\"\n",
    "    Computes the bitwise mutual information (BMI). See Sec.IV.D in [1] for details.\n",
    "    Note that if the input probabilities (log_app) are mismatched, this BMI estimation can be negative.\n",
    "\n",
    "    :param log_app: Logarithmic APP estimations of the symbol detector.\n",
    "    :param label_bits: The actually sent bits.\n",
    "    :returns BMI (scalar value)\n",
    "    \"\"\"\n",
    "    # Apply bit metric decoder to log APPs of symbols to get bit-wise LLRs.\n",
    "    assert log_app.shape[-2] == label_bits.shape[-1]/constellation.m\n",
    "    assert log_app.shape[-1] == constellation.M # The last shape should be the log-probabilities of the symbols.\n",
    "    llrs = constellation.bit_metric_decoder(log_app)\n",
    "    return constellation.m * (1 - t.mean(1/np.log(2) * (t.clamp((2*label_bits-1) * llrs, 0) + t.log(1+t.exp(-t.abs((2*label_bits-1) * llrs))))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure the training parameters here.**\n",
    "\n",
    "If you don't want to optimize any parameters, you can skip this cell.\n",
    "\n",
    "Note: \n",
    "* You might need to adapt the batch_size and the number of batches, depending on your specific transmission scenario and GAP parametrization to enable an effective optimization.\n",
    "* This implementation of the BMI does not correct a mismatching of the LLR values. It is thus possible for the BMI to be negative, if the LLRs are not based on 'real' probabilities, but e.g., on approximations of probabilities. There are ways to correct this (see [1, Sec. IV.D]), however, for training purposes, this implementation of the BMI is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMI: -1.255365  [      0/   1000]\n",
      "BMI: 0.411927  [    100/   1000]\n",
      "BMI: 0.742328  [    200/   1000]\n",
      "BMI: 0.794135  [    300/   1000]\n",
      "BMI: 0.852713  [    400/   1000]\n",
      "BMI: 0.882576  [    500/   1000]\n",
      "BMI: 0.918109  [    600/   1000]\n",
      "BMI: 0.928771  [    700/   1000]\n",
      "BMI: 0.932534  [    800/   1000]\n",
      "BMI: 0.945980  [    900/   1000]\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "EbN0_db_training_min = 10 # lower bound for uniform distribution of Eb/N0 in dB during the training\n",
    "EbN0_db_training_max = 10 # upper bound for uniform distribution of Eb/N0 in dB during the training\n",
    "batch_size = 100 # number of parallel blocks per training batch\n",
    "learning_rate = 0.001 # Learning rate of the optimizer.\n",
    "optimizer = t.optim.Adam(eq.parameters(), lr=learning_rate) # optimizer for paramter optimization\n",
    "batches = 1000 # number of batches to train on, e.g., the number of gradient descent steps\n",
    "\n",
    "# training loop, optimize parameters w.r.t. the BMI\n",
    "for batch_no in range(batches):\n",
    "    loss = - BMI(*(detect(EbN0_db_training_min, EbN0_db_training_max)), const) # minimize the negative BMI (i.e., maximize the BMI).\n",
    "    # optimization step(stochastic gradient descent)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Print training progress regularly.\n",
    "    if batch_no % 100 == 0:\n",
    "        print(f\"BMI: {-loss:>7f}  [{batch_no:>7d}/{batches:>7d}]\")\n",
    "\n",
    "# (optionally) save the optimization result to files\n",
    "#eq.save_weights(f\"path/to/directory/param_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure the evaluation of the GAP algorithm here:**\n",
    "\n",
    "Note:\n",
    "* You might need to adapt the batch_size and the number of batches, depending on your specific transmission scenario and GAP parametrization in order to get reliable approximations of the BMI and BER.\n",
    "* This implementation of the BMI does not correct a mismatching of the LLR values. It is thus possible for this estimation of the BMI to be negative, if the LLRs are not based on 'real' probabilities, but e.g., on approximations of probabilities. There are ways to correct this (scaling of the LLR values) (see [1, Sec. IV.D] for a more detailed discussion). To keep this notebook simple, we don't incluce this mismatch correction here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation for Eb/N0: tensor([ 0,  2,  4,  6,  8, 10, 12])\n",
      "BMI: tensor([0.4331, 0.5472, 0.6628, 0.7753, 0.8736, 0.9342, 0.9648],\n",
      "       device='cuda:0') \n",
      "BER: tensor([0.1743, 0.1440, 0.0927, 0.0675, 0.0279, 0.0152, 0.0110],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# eval config\n",
    "EbN0_dB_range = t.arange(0,13,2) # Range of Eb/N0 values (in dB) that we evaluate\n",
    "batch_size = 10**2 # number of parallel blocks for the evaluation\n",
    "\n",
    "# evaluate the optimized GAP algorithm w.r.t. the BER and BMI for some Eb/N0 values\n",
    "BER_over_EbN0 = t.empty(len(EbN0_dB_range), device=device) # prepare results tensor\n",
    "BMI_over_EbN0 = t.empty(len(EbN0_dB_range), device=device) # prepare results tensor\n",
    "with t.no_grad():\n",
    "    for i,EbN0_dB in enumerate(EbN0_dB_range):\n",
    "        BMI_over_EbN0[i] = BMI(*(detect(EbN0_dB, EbN0_dB)), const)\n",
    "        BER_over_EbN0[i] = BER(*(detect(EbN0_dB, EbN0_dB)), const)\n",
    "\n",
    "print(f\" Evaluation for Eb/N0: {EbN0_dB_range}\\nBMI: {BMI_over_EbN0} \\nBER: {BER_over_EbN0}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
